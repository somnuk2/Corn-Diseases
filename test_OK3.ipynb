{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF9uvbXNVrVY"
      },
      "source": [
        "## Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFBhRrrEI49z"
      },
      "source": [
        "It's good practice to use a validation split when developing your model. Let's use 80% of the images for training, and 20% for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "batch_size = 32\n",
        "img_height = 224\n",
        "img_width = 224\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "data_dir = \"D:/CNN-With/data/\"\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.9,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "class_names = val_ds.class_names\n",
        "print(class_names)\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "# Define the path where the model is saved (in H5 format)\n",
        "saved_model_path = \"D:/CNN-With/Trained_Model_Cnn/model.h5\"\n",
        "# Load the model using tf.keras.models.load_model()\n",
        "model = tf.keras.models.load_model(saved_model_path)\n",
        "# Print the model summary to verify it's loaded correctly\n",
        "model.summary()\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "val_image_batch, val_label_batch = next(iter(val_ds))\n",
        "true_label_ids = np.argmax(val_label_batch, axis=-1)\n",
        "print(\"Validation batch shape:\", val_image_batch.shape)\n",
        "# Assuming model and class_names are already defined\n",
        "# Get the predictions from the model\n",
        "tf_model_predictions = model.predict(val_image_batch)\n",
        "# If the model has logits as output (from_logits=True in loss), apply softmax to get probabilities\n",
        "if 'from_logits' in model.loss.get_config() and model.loss.get_config()['from_logits']:\n",
        "    tf_model_predictions = tf.nn.softmax(tf_model_predictions).numpy()\n",
        "# Convert predictions to a DataFrame\n",
        "tf_pred_dataframe = pd.DataFrame(tf_model_predictions)\n",
        "# Set the DataFrame columns to class names\n",
        "tf_pred_dataframe.columns = class_names\n",
        "# Print the first few predictions to check\n",
        "print(\"Prediction results for the first elements\")\n",
        "print(tf_pred_dataframe.head())\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "# Assuming class_names and val_ds are already defined\n",
        "# Initialize lists to store true labels and predictions\n",
        "all_true_labels = []\n",
        "all_predicted_classes_thresholded = []\n",
        "all_pred_confidences = []\n",
        "# Loop through the entire validation dataset\n",
        "for images, labels in val_ds:\n",
        "    # Get the true labels for this batch\n",
        "    all_true_labels.extend(labels.numpy())  # Store true labels\n",
        "    # Get the predictions from the model for this batch\n",
        "    tf_model_predictions = model.predict(images)\n",
        "    # If the model has logits as output (from_logits=True in loss), apply softmax to get probabilities\n",
        "    if 'from_logits' in model.loss.get_config() and model.loss.get_config()['from_logits']:\n",
        "        tf_model_predictions = tf.nn.softmax(tf_model_predictions).numpy()\n",
        "    # Convert predictions to a DataFrame\n",
        "    tf_pred_dataframe = pd.DataFrame(tf_model_predictions)\n",
        "    # Set the DataFrame columns to class names\n",
        "    tf_pred_dataframe.columns = class_names\n",
        "    # Apply thresholding to make predictions more confident\n",
        "    threshold = 0.5  # You can change this value\n",
        "    predicted_classes_thresholded = tf_pred_dataframe.apply(\n",
        "        lambda x: x.idxmax() if x.max() >= threshold else 'Uncertain', axis=1\n",
        "    )\n",
        "    # Store the predicted classes and prediction confidences\n",
        "    all_predicted_classes_thresholded.extend(predicted_classes_thresholded)\n",
        "    all_pred_confidences.extend(tf_pred_dataframe.max(axis=1))\n",
        "# Convert true labels to class names\n",
        "true_labels_class = [class_names[label] for label in all_true_labels]\n",
        "# Create a DataFrame to show both true and predicted labels for the entire dataset\n",
        "comparison_df = pd.DataFrame({\n",
        "    'True Label': true_labels_class,\n",
        "    'Predicted Label': all_predicted_classes_thresholded,\n",
        "    'Prediction Confidence': all_pred_confidences\n",
        "})\n",
        "# Print the first few rows of the comparison\n",
        "print(\"True vs Predicted Labels for all elements:\")\n",
        "print(comparison_df.head())\n",
        "# Optionally, you can save this DataFrame to a CSV to examine it later\n",
        "# comparison_df.to_csv(\"predictions_vs_true_labels.csv\", index=False)\n",
        "#----------------------------------------------------------------------------------------------------------------\n",
        "# Assuming class_names, true_labels_class, and all_predicted_classes_thresholded are already defined\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(true_labels_class, all_predicted_classes_thresholded, labels=class_names)\n",
        "# Convert confusion matrix to DataFrame for better readability\n",
        "cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "# Calculate Precision and Recall for each class\n",
        "precision = precision_score(true_labels_class, all_predicted_classes_thresholded, average=None, labels=class_names)\n",
        "recall = recall_score(true_labels_class, all_predicted_classes_thresholded, average=None, labels=class_names)\n",
        "# Calculate F1-Score for each class\n",
        "f1 = f1_score(true_labels_class, all_predicted_classes_thresholded, average=None, labels=class_names)\n",
        "# Create a DataFrame to show precision, recall, and f1-score\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Precision': precision,\n",
        "    'Recall': recall,\n",
        "    'F1-Score': f1\n",
        "}, index=class_names)\n",
        "# Print confusion matrix and metrics\n",
        "print(\"Confusion Matrix:\\n\")\n",
        "print(cm_df)\n",
        "print(\"\\nPrecision, Recall, and F1-Score for each class:\\n\")\n",
        "print(metrics_df)\n",
        "# Optionally, calculate average precision, recall, and F1-Score\n",
        "average_precision = precision_score(true_labels_class, all_predicted_classes_thresholded, average='macro')\n",
        "average_recall = recall_score(true_labels_class, all_predicted_classes_thresholded, average='macro')\n",
        "average_f1 = f1_score(true_labels_class, all_predicted_classes_thresholded, average='macro')\n",
        "# Calculate mean and standard deviation for Precision, Recall, and F1-Score\n",
        "mean_precision = np.mean(precision)\n",
        "mean_recall = np.mean(recall)\n",
        "mean_f1 = np.mean(f1)\n",
        "std_precision = np.std(precision)\n",
        "std_recall = np.std(recall)\n",
        "std_f1 = np.std(f1)\n",
        "# Print the averages, means, and standard deviations\n",
        "print(f\"\\nAverage Precision: {average_precision:.4f}\")\n",
        "print(f\"Average Recall: {average_recall:.4f}\")\n",
        "print(f\"Average F1-Score: {average_f1:.4f}\")\n",
        "print(f\"\\nMean Precision: {mean_precision:.4f}\")\n",
        "print(f\"Mean Recall: {mean_recall:.4f}\")\n",
        "print(f\"Mean F1-Score: {mean_f1:.4f}\")\n",
        "print(f\"\\nStandard Deviation of Precision: {std_precision:.4f}\")\n",
        "print(f\"Standard Deviation of Recall: {std_recall:.4f}\")\n",
        "print(f\"Standard Deviation of F1-Score: {std_f1:.4f}\")\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n",
        "# Plotting the metrics table as a heatmap (similar to confusion matrix)\n",
        "plt.figure(figsize=(8, 4))  # Adjust the figure size as needed\n",
        "# Use seaborn heatmap to display the metrics table as a styled heatmap\n",
        "sns.heatmap(metrics_df, annot=True, fmt='.4f', cmap='Blues', cbar=False, annot_kws={\"size\": 12}, \n",
        "            xticklabels=metrics_df.columns, yticklabels=metrics_df.index, linewidths=1, linecolor='black')\n",
        "# Customize the table appearance\n",
        "plt.title(\"Precision, Recall, and F1-Score for each Class\")\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Classes')\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "deep",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
